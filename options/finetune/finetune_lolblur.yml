# Finetuning configuration for DarkIR on LOLBlur dataset
# This configuration is specifically for finetuning on the LOLBlur dataset

#### GPU configuration
gpu: "0"

#### datasets
datasets:
  name: LOLBlur
  train:
    train_path: ./data/datasets/LOLBlur/train
    batch_size: 4  # Reduce if OOM
    cropsize: 256
    n_workers: 4
    verbose: True
  val:
    test_path: ./data/datasets/LOLBlur/test
    batch_size_test: 1

#### network structures
network:
  name: DarkIR
  pretrained_path: ./models/DarkIR_384.pt
  resume_training: False
  img_channels: 3
  width: 32
  middle_blk_num_enc: 2
  middle_blk_num_dec: 2
  enc_blk_nums: [1, 2, 3]
  dec_blk_nums: [3, 1, 1]
  dilations: [1, 4, 9]
  extra_depth_wise: True

#### training settings
train:
  epochs: 50
  lr_initial: 0.00005  # Lower learning rate for finetuning
  lr_scheme: CosineAnnealing
  eta_min: 0.000001
  weight_decay: 0.0001
  betas: [0.9, 0.999]
  use_side_loss: True

#### loss functions
losses:
  main_loss:
    type: CharbonnierLoss
    weight: 1.0
  
  ssim_loss:
    type: SSIMloss
    weight: 0.2
  
  enhance_loss:
    type: EnhanceLoss
    weight: 0.5

#### model saving
save:
  path: ./models/finetuned_lolblur

#### wandb logging
wandb:
  init: False
  project: DarkIR-Finetune
  entity: your_entity_name
  name: finetune_lolblur
  save_code: True
  resume: False
  id: null
  dir: ./wandb_logs
